<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Angus Galloway | Batch Norm is a Cause of Adversarial Vulnerability</title>
  <meta name="description" content="Personal research blog.
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/blog/2019/batch-norm-causes-vulnerability/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        Angus Galloway
        <!--<strong>Angus</strong> Galloway-->
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/publications/">publications</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/angus_galloway_cv.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\(', '\)'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>

<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>Link to <a href="https://arxiv.org/abs/1905.02161">arXiv Preprint</a>.</p>

<h1 id="abstract">Abstract</h1>

<p>Batch normalization is often used in an attempt to stabilize and
accelerate training in deep neural networks. In many cases it indeed decreases the
number of parameter updates required to reduce the training error. However,
it also reduces robustness to small input perturbations and noise by
double-digit percentages, as we show on five standard datasets.
Furthermore, substituting weight decay for batch norm is sufficient to nullify
the relationship between adversarial vulnerability and the input dimension.
Our work is consistent with a mean-field analysis that found that batch norm
causes exploding gradients.</p>

<h1 id="batch-norm">Batch Norm</h1>

<p>We briefly review how batch norm modifies the hidden layers’ pre-activations $h$
of a neural network. We use the notation
of <a href="#yang2019mean">(Yang, Pennington, Rao, Sohl-Dickstein, &amp; Schoenholz, 2019)</a>, where $\alpha$ is the index for a neuron, $l$
for the layer, and $i$ for a mini-batch of $B$ samples from the dataset;
$N_l$ denotes the number of neurons in layer $l$, $W^l$ is the matrix of weights
and $b^l$ is the vector of biases that parametrize layer $l$.
The batch mean is defined as $\mu_\alpha = \frac{1}{B} \sum_i h_{\alpha
i}$, and the variance is $\sigma_\alpha^2 = \sqrt{\frac{1}{B}
\sum_i {(h_{\alpha i} - \mu_\alpha)}^2  + c}$,
where $c$ is a small constant to prevent division by zero.
In the batch norm procedure, the mean $\mu_\alpha$ is subtracted from the
pre-activation of each neuron $h_{\alpha i}^l$ – consistent
with <a href="#ioffe2015batch">(Ioffe &amp; Szegedy, 2015)</a> –,
the result is divided by the standard deviation $\sigma_\alpha$, then scaled and shifted
by the learned parameters $\gamma_\alpha$ and $\beta_\alpha$, respectively.
This is described in Eqs. \eqref{eq:bn1} and \eqref{eq:bn2},
where a per-unit nonlinearity $\phi$,
e.g., ReLU, is applied after the normalization.</p>

<p>\begin{equation} \label{eq:bn1}
h_{\alpha i}^l = \gamma_{\alpha} \frac{h_{\alpha i} - \mu_{\alpha}}{\sigma_{\alpha}} + \beta_{\alpha}
\end{equation}</p>

<p>\begin{equation} \label{eq:bn2}
h_i^l = W^l \phi (h_i^{l - 1}) + b^l
\end{equation}</p>

<p>Note that this procedure fixes the first and second moments of all neurons $\alpha$
equally. This suppresses the information contained in these moments.
Batch norm induces a nonlocal batch-wise nonlinearity, such that
two mini-batches that differ by only a <em>single</em> example will have
different representations for <em>each</em> example <a href="#yang2019mean">(Yang, Pennington, Rao, Sohl-Dickstein, &amp; Schoenholz, 2019)</a>.
This difference is further amplified by stacking batch norm layers.
We argue that this information loss and inability to maintain relative
distances in the input space reduces adversarial as well as general robustness.</p>

<p align="center">
  <img width="216" height="216" src="/img/bn_animation.gif" />
</p>
<p>We illustrate this effect by reproducing Fig. 6 of
<a href="#yang2019mean">(Yang, Pennington, Rao, Sohl-Dickstein, &amp; Schoenholz, 2019)</a> in the above animation.
Two mini-batches that contain
the same data points except for one are shown at Layer 0, or input.
We propagate the mini-batches through a deep batch-normalized linear network,
i.e. with $\phi=id$, and of any practical width. The activations are then
projected to their two principal components. This figure reminded me of
the Adversarial Spheres dataset for binary classification of concentric
spheres on the basis of their differing radii
<a href="#gilmer2018adversarial">(Gilmer et al., 2018)</a>. It turns out that this
simple task poses a challenge to the conventional wisdom that batch norm
accelerates training and improves generalization; batch norm does the exact
opposite in this case, prolonging training by $\approx 50 \times$, increasing
sensitivity to the learning rate, and reduces robustness. This is likely why
they used the Adam optimizer instead of plain SGD in the original work on
Adversarial Spheres, and trained in an online manner for one million steps.
The next illustration makes it clear why this is so.</p>

<!--<p align="center">
  <img width="600" height="600" src="/img/LinearNet_InitHe_L60_W10_BNTrue_ModeTrain_Seed3_MB32.png">
</p>-->

<p align="center">
  <img width="432" height="158" src="/img/AdvSpheresNoNoise3x_LinearNet_InitHe_L30_W10_BNTrue_ModeTrain_Seed3_MB32_EdgeColorsBatch-png.png" />
</p>

<p>Concentric circles and their representations in a deep
linear network with batch norm at initialization.
Mini-batch membership is indicated by marker fill and class membership
by colour. Each layer is projected to its two principal components.
Some samples overlap at Layer 2, and classes are mixed at Layer 14.</p>

<p align="center">
  <img width="432" height="468" src="/img/mft_batch_norm_tex_cbar_bottom_att_bn_theoretical_png.png" />
</p>
<p>In the next visualization, we repeat the experiment of <a href="#yang2019mean">(Yang, Pennington, Rao, Sohl-Dickstein, &amp; Schoenholz, 2019)</a> by training fully-connected nets of depth $L$ and
constant-width ReLU layers for ten
epochs by SGD, and learning rate $\eta = 10^{-5} B$ for batch size $B$ on MNIST.
The batch norm parameters $\gamma$ and $\beta$ were left as default, momentum was
disabled, and $c = 10^{-3}$. Trials were averaged over three random seeds.</p>

<p>It turns out that one can predict the theoretical maximum depth solely as a
function of the batch size, due to an – almost paradoxically reliable –
gradient explosion due to batch norm.
The following function computes this, up to a correction
factor since the general form works for <em>any</em> dataset, learning rate, or
optimizer. The dashed line shows the theoretical maximum trainable depth
in the context of our experiment.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Thm. 3.10 Yang et al., (2019)</span>
<span class="k">def</span> <span class="nf">J_1</span><span class="p">(</span><span class="n">c</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">arccos</span><span class="p">(</span><span class="n">c</span><span class="p">))</span><span class="o">*</span><span class="n">c</span><span class="p">)</span>

<span class="c"># first derivative of J_1</span>
<span class="k">def</span> <span class="nf">J_1_prime</span><span class="p">(</span><span class="n">c</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>

<span class="n">lambda_G</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span> <span class="o">-</span> <span class="mi">3</span><span class="p">))</span> <span class="o">*</span> <span class="p">((</span><span class="n">B</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">J_1_prime</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">J_1</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lambda_G</span><span class="p">)))</span> <span class="c"># 16 is a correction factor</span>
</code></pre></div></div>

<h1 id="references">References</h1>

<ol class="bibliography"><li>

<div id="yang2019mean">
  
    <span class="title">A Mean Field Theory of Batch Normalization</span>
    <span class="author">
      
        
          
            
              
                Yang, Greg,
              
            
          
        
      
        
          
            
              
                Pennington, Jeffrey,
              
            
          
        
      
        
          
            
              
                Rao, Vinay,
              
            
          
        
      
        
          
            
              
                Sohl-Dickstein, Jascha,
              
            
          
        
      
        
          
            
              
                and Schoenholz, Samuel S.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Learning Representations</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="gilmer2018adversarial">
  
    <span class="title">Adversarial Spheres</span>
    <span class="author">
      
        
          
            
              
                Gilmer, Justin,
              
            
          
        
      
        
          
            
              
                Metz, Luke,
              
            
          
        
      
        
          
            
              
                Faghri, Fartash,
              
            
          
        
      
        
          
            
              
                Schoenholz, Sam,
              
            
          
        
      
        
          
            
              
                Raghu, Maithra,
              
            
          
        
      
        
          
            
              
                Wattenberg, Martin,
              
            
          
        
      
        
          
            
              
                and Goodfellow, Ian
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Learning Representations Workshop Track</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li>
<li>

<div id="ioffe2015batch">
  
    <span class="title">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</span>
    <span class="author">
      
        
          
            
              
                Ioffe, Sergey,
              
            
          
        
      
        
          
            
              
                and Szegedy, Christian
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Machine Learning</em>
    
    
      2015
    
    </span>
  

  <span class="links">
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
</div>
</li></ol>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2021 .
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
